{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c17375-fa4c-48a9-a8cc-d2582b22e223",
   "metadata": {},
   "source": [
    "# Bonus Question\n",
    "Fit a neural network to the Default data. Use a single hidden layer with 10 units, and dropout regularization. Have a look at Labs 10.9.1–10.9.2 for guidance. Compare the classification performance of your model with that of linear logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ced094-0497-459b-95b1-fd3690b87117",
   "metadata": {},
   "source": [
    "## Train-Test Split and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67517580-4ceb-4c09-bbb3-1dd6236944ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"Default.csv\", index_col=0)\n",
    "\n",
    "df['default'] = (df['default'] == 'Yes').astype(int)\n",
    "df['student'] = (df['student'] == 'Yes').astype(int)\n",
    "\n",
    "X = df[['student', 'balance', 'income']]\n",
    "y = df['default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04da87-25b5-4efe-8dbe-c484d8376afb",
   "metadata": {},
   "source": [
    "## Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "389b0c84-8a0e-457a-b3ce-94778e21462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10, input_shape=(X_train_scaled.shape[1],), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce2b95-7faa-408e-a714-8b1ac0940e1e",
   "metadata": {},
   "source": [
    "## Model Evaluation - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51eb8769-c737-4916-a86e-0e390227c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step\n",
      "Neural Network Performance\n",
      "[[1928    5]\n",
      " [  51   16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      1933\n",
      "           1       0.76      0.24      0.36        67\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.87      0.62      0.67      2000\n",
      "weighted avg       0.97      0.97      0.96      2000\n",
      "\n",
      "AUC: 0.948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "y_pred_prob_nn = model.predict(X_test_scaled).flatten()\n",
    "y_pred_nn = (y_pred_prob_nn > 0.5).astype(int)\n",
    "\n",
    "print(\"Neural Network Performance\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred_prob_nn):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877505b-ecb0-4fdf-82f2-8b63d505a458",
   "metadata": {},
   "source": [
    "The neural network model shows a high overall classification accuracy of 97% and an excellent AUC of 0.948, indicating strong discriminatory power between the two classes. However, the performance metrics reveal a significant class imbalance problem. The model achieves near-perfect precision and recall for the majority class (non-default, class 0), but it struggles to correctly identify the minority class (default, class 1). Specifically, it only captures 24% of actual defaults (recall), which is concerning in applications like credit risk modeling where identifying defaulters is critical. The F1-score for class 1 is just 0.36, reflecting a poor balance between precision and recall for that class. This suggests that the model is biased toward predicting the majority class and is failing to capture the rare but important instances of default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e361ad3-139d-4b22-b3f1-513e686a9671",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bffbce4d-0c15-4484-bee4-ae5f2baaad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance\n",
      "[[1925    8]\n",
      " [  46   21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1933\n",
      "           1       0.72      0.31      0.44        67\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.85      0.65      0.71      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n",
      "AUC: 0.948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_prob_lr = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_lr = (y_pred_prob_lr > 0.5).astype(int)\n",
    "\n",
    "print(\"Logistic Regression Performance\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred_prob_lr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81efef-8456-428f-b498-71e3db020a7e",
   "metadata": {},
   "source": [
    "The logistic regression model, like the neural network, achieves a high overall accuracy of 97% and an identical AUC of 0.948. This indicates that both models are equally effective at distinguishing between the default and non-default classes when evaluated using the area under the ROC curve. However, a closer look at the classification metrics again reveals the challenge posed by class imbalance. The logistic regression model performs very well on the majority class (non-default), with near-perfect precision and recall. For the minority class (default), the model achieves a recall of 31%, slightly better than the neural network's 24%, and a precision of 72%, which is comparable. Its F1-score for the minority class is 0.44, outperforming the neural network's 0.36 and indicating a better balance between precision and recall in identifying defaulters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492ad7c-f238-4fff-a026-ac229f95150c",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c364c0-17d5-46b3-a544-5d9cdcef5a21",
   "metadata": {},
   "source": [
    "Both models achieve the same overall accuracy of 97% and an identical AUC of 0.948, indicating strong and comparable ability to distinguish between defaulters and non-defaulters. However, important differences emerge when examining performance on the minority class (defaults). The neural network correctly identifies only 24% of defaulters (recall), with a corresponding F1-score of 0.36, despite having high precision (76%). This indicates that the model is conservative in predicting defaults and fails to detect most of them. In contrast, the logistic regression model achieves a recall of 31% and an F1-score of 0.44 for the same class, outperforming the neural network in its ability to capture actual default cases. While both models perform well on the majority class (non-default), logistic regression demonstrates slightly better balance between precision and recall for the minority class, making it the preferable choice in this case, especially when the ability to identify defaults is more critical than overall accuracy. A possible explanation could be that the neural network’s complexity may have caused it to overfit to the dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f0251-f8ac-4cbb-91ff-e6f8ff02dc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
